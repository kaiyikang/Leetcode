## 什么是样本不平衡？

从直觉上讲，如果某一类别的数据量，远大于另外一个类别的数据量，我们称其为样本不平衡。例如异常检测，因为绝大部分的样本都是正常的，只有少部分是异常的。

如果从 $Accuracy$ 和 $Recall$ 的角度上看，正常样本的 $Recall$ 非常高，异常样本的 $Recall$ 非常低，但是分类器整体的 $Accuracy$ 很高，这就出现了问题。

在这里，$Recall = \frac{TP}{P}$ 可以理解成，判定真类位真的比率。$Accuracy=\frac{TP+TN}{P+N}$ 则是所有类别判断正确的比率。

## 样本不平衡的解决方案？

可以使用 过采样、欠采样、使用集成方法 等方式解决。

### 欠采样 under-sampling

从数据角度看，我们可以减少大类中的样本数量，让大类的样本数和小类的样本数相似。

我们可以从大类中随机挑选一些样本，和小类组成新的样本，缺点是会导致信息丢失。

针对该缺点，能够做出如下改进。假设小类中共有N个样本，我们将大类的样本分成N个堆堆（簇），然后取簇中的中点、平均点等，能够代表该簇的点，并把它们全部收集起来，和小类组成新的数据集。

### 过采样 over-sampling

过采样是对数据集内的小类样本进行扩充，使得小类样本数量和大类样本数量近似，然后学习。

如果只是随机的复制小类样本，这样会非常容易产生模型的过拟合，模型学习到的信息非常特殊，而不够泛化。

于是，我们可以使用SMOTE方法（Synthetic Minority Oversampling Technique）缓解此类问题的发生。

SMOTE方法中心思想是，以欧氏距离为标准，寻找点 x 周围 k 个临近的点，随机选择其中一个临近点，并通过公式 $x_{new}=x+rand(0,1)\cdot(\hat{x}-x)$ ，其中 $\hat{x}$ 是临近点。

当然，SMOTE方法也并非完美，k值的选择会对新样本的生成有很大影响。另外如果小类样本非常靠近大类样本的边界，那么生成的新样本就容易生成在边界，给分类造成困难。

### 集成方法

集成学习中，我们训练多个『弱』模型去解决相同的问题，并结合起来以获得更好的结果。这里的假设是，弱模型的正确组合，可以得到更加精确/鲁棒的模型。

- bagging方法，同质弱学习器，相互独立并行训练弱学习器，并按照确定性的平均过程将它们组合起来。
- boosting方法，同直弱学习器，自实应的方法按照顺序训练弱学习器，即每个模型的训练都依赖于前面的模型，并最后按照某种确定的策略将它们组合起来。
- stacking方法，异质弱学习器。

























































https://zhuanlan.zhihu.com/p/57153934

